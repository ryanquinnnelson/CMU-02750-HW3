{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, WhiteKernel,ExpSineSquared,DotProduct,RationalQuadratic\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# scipy\n",
    "from scipy import sparse\n",
    "\n",
    "# custom packages\n",
    "from packages.gridsearch import gridsearch as gs\n",
    "\n",
    "\n",
    "\n",
    "### Set random seed\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "### Suppresses Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/hw3_data.csv', delimiter=',',header=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate columns for each amino acid\n",
    "for i in range(9): # all seq are length 9\n",
    "    colname='seq'+str(i)\n",
    "    data[colname] = [x[i] for x in data['seq']]\n",
    "\n",
    "# separate features and target, remove unnecessary columns\n",
    "X_df = data.drop(['pIC50','id','allele', 'seq'],axis=1)\n",
    "y = data['pIC50']\n",
    "\n",
    "# encode features\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_enc = enc.fit_transform(X_df)\n",
    "\n",
    "# ?? standardize target\n",
    "\n",
    "# convert to numpy array ?? is this necessary\n",
    "X_pool=sparse.csr_matrix.toarray(X_enc)\n",
    "y_pool = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6064, 180)\n",
      "(6064,)\n",
      "(2987, 180)\n",
      "(2987,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pool, y_pool, test_size=0.33)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments with offline learners and kernels\n",
    "To see if a particular regressor or kernel works better on the data than any other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest Regressor\n",
    "Doesn't meet 0.6 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # train RFC model on entire pool of data\n",
    "# rf = RandomForestRegressor(n_estimators = 20, \n",
    "#                             max_depth = 6, \n",
    "#                             random_state = seed)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# # calculate accuracy\n",
    "# print(rf.score(X_test,y_test)) #0.476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ridge Regression\n",
    "Barely passable to meet 0.6 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# clf = Ridge(alpha=1.0)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# print(clf.score(X_test,y_test))  #0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ridge Regression with variable alpha\n",
    "Did not dramatically improve score. (0.63 -> 0.65 maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid search had no significant improvement \n",
    "# for i in np.linspace(0.1,5,50):\n",
    "#     clf = Ridge(alpha=i)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     print(np.round(i,2),np.round(clf.score(X_test,y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 RBF Kernel\n",
    "Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #2 min\n",
    "\n",
    "# # checking if WhiteKernel is helping or not\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3))\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.6885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RBF + WhiteKernel\n",
    "WhiteKernel doesn't seem to improve score. Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 15 minutes for (5931,180)\n",
    "\n",
    "# check if WhiteKernel helps or not\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "#          + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.686 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 RBF no length_scale_bounds\n",
    "Seems to increase runtime slightly. No effect on results. Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# # checking if bounds is helping or not\n",
    "# kernel = RBF(length_scale=1.0)\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test))  #0.6885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 RationalQuadratic kernel\n",
    "Good score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 3.5 min\n",
    "\n",
    "# kernel = RationalQuadratic(length_scale=1.0, alpha=1.5, length_scale_bounds=(1e-2, 1e3))\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.6888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 DotProduct + WhiteKernel\n",
    "Barely passable score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# kernel = DotProduct() + WhiteKernel()\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) # 0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Default Kernel\n",
    "Bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 15 s\n",
    "\n",
    "# gpr = GaussianProcessRegressor(random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) # -0.806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 RBF + ExpSineSquared\n",
    "Idea is that sequences are \"periodic\" data, and sequences repeat in groups. If we combine RBF and periodic kernel we could better model the data. Model wouldn't run due to LinAlgError:\n",
    "\n",
    "```\n",
    "LinAlgError: (\"The kernel, RBF(length_scale=1) + ExpSineSquared(length_scale=1, periodicity=1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '10-th leading minor of the array is not positive definite')\n",
    "```\n",
    "\n",
    "Attempted to modify alpha to fix issue, to no avail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from numpy.linalg import LinAlgError\n",
    "\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + ExpSineSquared(length_scale=1, periodicity=1)\n",
    "\n",
    "# for alpha in [1E-9,1E-8,1E-7,1E-6,1E-5,1E-4,1E-3,1E-2,1E-1,1E-0]:\n",
    "        \n",
    "# #     try:\n",
    "# #         gpr = GaussianProcessRegressor(kernel,random_state=seed, alpha=alpha)\n",
    "# #         gpr.fit(X_train, y_train)\n",
    "# #         print(alpha, gpr.score(X_test,y_test))  \n",
    "# #     except LinAlgError:\n",
    "# #         print(alpha, \"Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Matern\n",
    "Good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# kernel = Matern(length_scale=1.0, nu=1.5)\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) # 0.687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Active Learning model - Grid Search v1\n",
    "Experiments to test out various combinations to achieve $R^2 \\ge 0.6$ threshold.\n",
    "\n",
    "All experiments in this section use the following configurations:\n",
    "- Category: Committee\n",
    "- Learner: Gaussian Process\n",
    "- query_strategy: max_std_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?? Cookbook suggests \"put a product of SE kernels on those dimensions\" (I have 180 dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_details(kernel):\n",
    "    return [kernel,kernel.length_scale, kernel.nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_details(kernel):\n",
    "    return [kernel, kernel.length_scale, kernel.length_scale_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationalquadratic_details(kernel):\n",
    "    return [kernel, kernel.length_scale, kernel.alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_plus_white(kernel):\n",
    "    return [kernel, kernel.k1.length_scale, kernel.k2.noise_level]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Matern\n",
    "- 26 minutes with `2*2*10` Matern configurations at 100 queries - 2,5 learners\n",
    "- 3 hours 4 minutes with `2*2*10` Matern configurations at 100 queries - 10,20 learners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'nu', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [Matern(length_scale=i, nu=1.5) for i in np.linspace(0.5,1,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, matern_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.38,0.418,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. RationalQuadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'alpha', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RationalQuadratic(length_scale=i, alpha=j, length_scale_bounds=(1e-2, 1e3)) \n",
    "#            for i in np.linspace(0.5,10,2) \n",
    "#            for j in np.linspace(0.5,2,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rationalquadratic_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. RBF + WhiteKernel random search\n",
    "Stopped halfway through because results weren't interesting. Adding noise doesn't improve score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','rbf_length_scale', 'white_noise_scale', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=0.45,length_scale_bounds=(1e-2, 1e3)) \\\n",
    "#            + WhiteKernel(noise_level=i, noise_level_bounds=(1e-10, 1e+1)) \n",
    "#            for i in np.random.uniform(low=0.1,high=1.0,size=20)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_plus_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. RBF linear search wide\n",
    "Search for peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in [1e-10,1e-5,1e-2,1e-1,1,1e1,100]]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. RBF linear search narrow\n",
    "Search near current peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1 hour\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in np.linspace(0.1,1,10)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(1,10,10)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. RBF linear search narrow 2\n",
    "Search near current peaks. Skipped fifth run due to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. RBF linear search narrow 3\n",
    "Search near current peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 3h 4 min\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.45,0.55,10)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.7,0.8,10)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(2.4,3.0,10)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(3.7,4.0,10)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(4.8,5.2,10)]  \n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. RBF random search narrow\n",
    "Randomized points near peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['3.9']\n",
    "# row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['experiment','category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.random.uniform(low=2.4,high=3.0,size=20)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.random.uniform(low=3.8,high=4.0,size=20)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.random.uniform(low=4.8,high=5.0,size=20)] \\\n",
    "#         + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.random.uniform(low=0.49,high=0.51,size=20)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_1(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. RBF best points\n",
    "Checking rounded values near the highest R2 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['3_10']\n",
    "# row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['experiment','category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3))\n",
    "#            for i in [0.500,0.738,1.923,3.785,5.985,2.769,2.760,2.412,4.983]]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_1(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11. Matern wide search - scale_length\n",
    "Checking range of Matern values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['3_11']\n",
    "# row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['experiment','category','learner','query_strategy','kernel','length_scale', 'nu', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = kernels = [Matern(length_scale=i, nu=1.5) for i in [1e-10,1e-5,1e-2,1e-1,1,1e1,100]]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_1(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, matern_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12. RBF Random Search ?\n",
    "Add additional points to clarify score shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['3_10']\n",
    "# row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['experiment','category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.random.uniform(low=0.4,high=5.1,size=200)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_1(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Active learning model - Grid Search v2\n",
    "Experiments to test out various combinations to achieve highest starting $R^2$ value.\n",
    "\n",
    "All experiments in this section use the following configurations:\n",
    "- Category: Single Learner\n",
    "- Learner: Random Forest Regressor\n",
    "- query_strategy: GP_regression_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Grid Search over hyperparameters\n",
    "Find a good starting configuration for Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['4_1']\n",
    "# row_prefix = ex_id + ['Single Learner','Random Forest Regressor','GP_regression_std']\n",
    "# fields = ['experiment','category','learner','query_strategy','n_estimators','max_depth','n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "\n",
    "# # configs\n",
    "# max_depths = [i for i in range(1,20)]\n",
    "# n_estimators = [i for i in range(1,100,5)]\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_2(n_estimators, max_depths, n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, randomforestdetails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Grid Search over hyperparameters\n",
    "Find a good starting configuration for Random Forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['4_2']\n",
    "# row_prefix = ex_id + ['Single Learner','Random Forest Regressor','GP_regression_std']\n",
    "# fields = ['experiment','category','learner','query_strategy','n_estimators','max_depth','n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "\n",
    "# # configs\n",
    "# max_depths = [i for i in range(1,20)]\n",
    "# n_estimators = [i for i in range(100,500,25)]\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_2(n_estimators, max_depths, n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, randomforestdetails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Grid Search over hyperparameters\n",
    "Repeat to reduce variance in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# 22min\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['4_3']\n",
    "# row_prefix = ex_id + ['Single Learner','Random Forest Regressor','GP_regression_std']\n",
    "# fields = ['experiment','category','learner','query_strategy','n_estimators','max_depth','n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "\n",
    "# # configs\n",
    "# max_depths =  [i for i in range(1,20)] \\\n",
    "#             + [i for i in range(1,20)] \\\n",
    "#             + [i for i in range(1,20)] \\\n",
    "#             + [i for i in range(1,20)]\n",
    "\n",
    "# n_estimators = [i for i in range(1,100,5)] \\\n",
    "#              + [i for i in range(1,100,5)] \\\n",
    "#              + [i for i in range(1,100,5)] \\\n",
    "#              + [i for i in range(1,100,5)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gs.grid_search_2(n_estimators, max_depths, n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
