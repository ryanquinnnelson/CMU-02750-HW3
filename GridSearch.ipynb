{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# numpy\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "# sklearn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, WhiteKernel,ExpSineSquared,DotProduct,RationalQuadratic\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modAL\n",
    "from modAL.disagreement import max_std_sampling\n",
    "from modAL.models import ActiveLearner, CommitteeRegressor\n",
    "\n",
    "# scipy\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "# from modAL.models import BayesianOptimizer, \n",
    "# from modAL.acquisition import max_EI\n",
    "\n",
    "\n",
    "### Set random seed\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "### Suppresses Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/hw3_data.csv', delimiter=',',header=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate columns for each amino acid\n",
    "for i in range(9): # all seq are length 9\n",
    "    colname='seq'+str(i)\n",
    "    data[colname] = [x[i] for x in data['seq']]\n",
    "\n",
    "# separate features and target, remove unnecessary columns\n",
    "X_df = data.drop(['pIC50','id','allele', 'seq'],axis=1)\n",
    "y = data['pIC50']\n",
    "\n",
    "# encode features\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_enc = enc.fit_transform(X_df)\n",
    "\n",
    "# ?? standardize target\n",
    "\n",
    "# convert to numpy array ?? is this necessary\n",
    "X_pool=sparse.csr_matrix.toarray(X_enc)\n",
    "y_pool = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6064, 180)\n",
      "(6064,)\n",
      "(2987, 180)\n",
      "(2987,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pool, y_pool, test_size=0.33)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiments with offline learners and kernels\n",
    "To see if a particular regressor or kernel works better on the data than any other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest Regressor\n",
    "Doesn't meet 0.6 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train RFC model on entire pool of data\n",
    "rf = RandomForestRegressor(n_estimators = 20, \n",
    "                            max_depth = 6, \n",
    "                            random_state = seed)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "print(rf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ridge Regression\n",
    "Barely passable to meet 0.6 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# clf = Ridge(alpha=1.0)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# print(clf.score(X_test,y_test))  #0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ridge Regression with variable alpha\n",
    "Did not dramatically improve score. (0.63 -> 0.65 maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grid search had no significant improvement \n",
    "# for i in np.linspace(0.1,5,50):\n",
    "#     clf = Ridge(alpha=i)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     print(np.round(i,2),np.round(clf.score(X_test,y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 RBF Kernel\n",
    "Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #2 min\n",
    "\n",
    "# # checking if WhiteKernel is helping or not\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3))\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.6885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RBF + WhiteKernel\n",
    "WhiteKernel doesn't seem to improve score. Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 15 minutes for (5931,180)\n",
    "\n",
    "# check if WhiteKernel helps or not\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "#          + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.686 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 RBF no length_scale_bounds\n",
    "Seems to increase runtime slightly. No effect on results. Good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# # checking if bounds is helping or not\n",
    "# kernel = RBF(length_scale=1.0)\n",
    "\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test))  #0.6885"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 RationalQuadratic kernel\n",
    "Good score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 3.5 min\n",
    "\n",
    "# kernel = RationalQuadratic(length_scale=1.0, alpha=1.5, length_scale_bounds=(1e-2, 1e3))\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) #0.6888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 DotProduct + WhiteKernel\n",
    "Barely passable score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2 min\n",
    "\n",
    "# kernel = DotProduct() + WhiteKernel()\n",
    "# gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) # 0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Default Kernel\n",
    "Bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 15 s\n",
    "\n",
    "# gpr = GaussianProcessRegressor(random_state=seed)\n",
    "# gpr.fit(X_train, y_train)\n",
    "# print(gpr.score(X_test,y_test)) # -0.806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 RBF + ExpSineSquared\n",
    "Idea is that sequences are \"periodic\" data, and sequences repeat in groups. If we combine RBF and periodic kernel we could better model the data. Model wouldn't run due to LinAlgError:\n",
    "\n",
    "```\n",
    "LinAlgError: (\"The kernel, RBF(length_scale=1) + ExpSineSquared(length_scale=1, periodicity=1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '10-th leading minor of the array is not positive definite')\n",
    "```\n",
    "\n",
    "Attempted to modify alpha to fix issue, to no avail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + ExpSineSquared(length_scale=1, periodicity=1)\n",
    "\n",
    "# for alpha in [1E-9,1E-8,1E-7,1E-6,1E-5,1E-4,1E-3,1E-2,1E-1,1E-0]:\n",
    "        \n",
    "# #     try:\n",
    "# #         gpr = GaussianProcessRegressor(kernel,random_state=seed, alpha=alpha)\n",
    "# #         gpr.fit(X_train, y_train)\n",
    "# #         print(alpha, gpr.score(X_test,y_test))  \n",
    "# #     except LinAlgError:\n",
    "# #         print(alpha, \"Error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Matern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "kernel = Matern(length_scale=1.0, nu=1.5)\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel,random_state=seed)\n",
    "gpr.fit(X_train, y_train)\n",
    "print(gpr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Active Learning model - Grid Search v1\n",
    "Experiments to test out various combinations to achieve $R^2 \\ge 0.6$ threshold.\n",
    "\n",
    "All experiments in this section use the following configurations:\n",
    "- Category: Committee\n",
    "- Learner: Gaussian Process\n",
    "- query_strategy: max_std_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?? Cookbook suggests \"put a product of SE kernels on those dimensions\" (I have 180 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sample(learner, X, y):\n",
    "    \n",
    "    # call the query strategy defined in the learner to obtain a new sample\n",
    "    query_idx, query_sample = learner.query(X)\n",
    "    \n",
    "    # modify indexing to interpret as collection of one element with d features\n",
    "    query_sample_reshaped = query_sample.reshape(1,-1)\n",
    "   \n",
    "    # obtain the query label\n",
    "    query_label = y[query_idx]\n",
    "\n",
    "    # modify indexing to interpret as 1D array of one element\n",
    "    query_label_reshaped = query_label.reshape(1,)\n",
    "    \n",
    "    return query_sample_reshaped, query_label_reshaped, query_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_learner_regression(learner, X_pool, y_pool, X_test, y_test, n_queries):\n",
    "\n",
    "    # perform active learning\n",
    "    for q in range(n_queries):\n",
    "\n",
    "        # get sample\n",
    "        X_sample, y_sample, query_idx = get_next_sample(learner, X_pool, y_pool)\n",
    "\n",
    "        # use new sample to update the model\n",
    "        learner.teach(X_sample, y_sample)\n",
    "        \n",
    "        # remove labeled instance from pool\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_file(filename,fields=None,rows=None):\n",
    "    \n",
    "    with open(filename,'a') as f:\n",
    "        \n",
    "        write = csv.writer(f)  # using csv.writer method from CSV package \n",
    "\n",
    "        if fields:\n",
    "            write.writerow(fields) \n",
    "        \n",
    "        if rows:\n",
    "            write.writerows(rows) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_committee(kernel, n_learner, n_initial, X_pool, y_pool, seed):\n",
    "    \n",
    "    # get initial training set for each learner\n",
    "    initial_idx = []\n",
    "    for i in range(n_learner):\n",
    "        initial_idx.append(np.random.choice(len(X_pool), size=n_initial, replace=False))\n",
    "\n",
    "    # initialize learners for Committee\n",
    "    learner_list = [ActiveLearner(\n",
    "        estimator=GaussianProcessRegressor(kernel,random_state=seed),\n",
    "        X_training=X_pool[idx],\n",
    "        y_training=y_pool[idx]) for idx in initial_idx]\n",
    "\n",
    "    # create Committee\n",
    "    committee = CommitteeRegressor(learner_list=learner_list,query_strategy=max_std_sampling)\n",
    "    \n",
    "    return commitee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process_regressor_gs(kernels, n_learners, n_initials, X_pool, y_pool, X_test, y_test, n_queries,\n",
    "                                  seed, filename, fields, row_prefix, details_func):\n",
    "    '''\n",
    "    Grid search over (kernels, n_learners, n_initials). Saves results to file.\n",
    "    Uses CommitteeRegressor for active learner with GaussianProcessRegressors as members.\n",
    "    '''\n",
    "    # append fields as first row to file\n",
    "    write_results_to_file(filename, fields)\n",
    "    \n",
    "    # perform grid search\n",
    "    for kernel in kernels:\n",
    "        for n_learner in n_learners:\n",
    "            for n_initial in n_initials:\n",
    "                \n",
    "                # make a copy of the data for use in this test\n",
    "                X_pool_gs = copy.deepcopy(X_pool)\n",
    "                y_pool_gs = copy.deepcopy(y_pool)\n",
    "\n",
    "                # get initial training set for each learner\n",
    "                initial_idx = []\n",
    "                for i in range(n_learner):\n",
    "                    initial_idx.append(np.random.choice(len(X_pool_gs), size=n_initial, replace=False))\n",
    "                \n",
    "                # initialize learners for Committee\n",
    "                learner_list = [\n",
    "                    ActiveLearner(\n",
    "                        estimator=GaussianProcessRegressor(kernel,random_state=seed),\n",
    "                        X_training=X_pool_gs[idx],\n",
    "                        y_training=y_pool_gs[idx]\n",
    "                    ) for idx in initial_idx]\n",
    "                    \n",
    "                # create Committee\n",
    "                committee = CommitteeRegressor(\n",
    "                                learner_list=learner_list,\n",
    "                                query_strategy=max_std_sampling\n",
    "                            )\n",
    "\n",
    "                # perform active learning\n",
    "                run_active_learner_regression(committee, X_pool_gs, y_pool_gs, X_test, y_test, n_queries)\n",
    "\n",
    "                # score model\n",
    "                y_pred = committee.predict(X_test, return_std=False)\n",
    "                r2=r2_score(y_test,y_pred)\n",
    "                \n",
    "                # create row for file\n",
    "                kernel_details = details_func(kernel)\n",
    "                meta = [n_learner, n_initial, n_queries, r2]\n",
    "                row = row_prefix + kernel_details + meta\n",
    "\n",
    "                # append to file\n",
    "                write_results_to_file(filename, rows=[row])\n",
    "                \n",
    "                # output to console for tracking progress\n",
    "                print('{}|{}|{}|{}|{}'.format(kernel, n_learner, n_initial,n_queries, r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_1(kernels, n_learners, n_initials, X_pool, y_pool, X_test, y_test, n_queries,\n",
    "                                  seed, filename, fields, row_prefix, details_func):\n",
    "    '''\n",
    "    Grid search over (kernels, n_learners, n_initials). \n",
    "    Appends results to file after each run combination. Experiment can be halted at any time without losing results.\n",
    "    Uses CommitteeRegressor for active learner with GaussianProcessRegressors as members.\n",
    "    '''\n",
    "    # append fields as first row in file\n",
    "    write_results_to_file(filename, fields)\n",
    "    \n",
    "    # perform grid search\n",
    "    for kernel in kernels:\n",
    "        for n_learner in n_learners:\n",
    "            for n_initial in n_initials:\n",
    "                \n",
    "                # make a copy of the data for use in this experiment\n",
    "                X_pool_gs = copy.deepcopy(X_pool)\n",
    "                y_pool_gs = copy.deepcopy(y_pool)\n",
    "\n",
    "                # build learner\n",
    "                committee = build_committee(kernel,n_learner, n_initial, X_pool_gs, y_pool_gs, seed)\n",
    "\n",
    "                # perform active learning\n",
    "                run_active_learner_regression(committee, X_pool_gs, y_pool_gs, X_test, y_test, n_queries)\n",
    "\n",
    "                # score model\n",
    "                y_pred = committee.predict(X_test, return_std=False)\n",
    "                r2=r2_score(y_test,y_pred)\n",
    "                \n",
    "                # create row for file\n",
    "                kernel_hyperparms = details_func(kernel)\n",
    "                meta = [n_learner, n_initial, n_queries, r2]\n",
    "                row = row_prefix + kernel_hyperparms + meta\n",
    "\n",
    "                # append to file\n",
    "                write_results_to_file(filename, rows=[row])\n",
    "                \n",
    "                # output to console for tracking progress\n",
    "                print('{}|{}|{}|{}|{}'.format(kernel, n_learner, n_initial,n_queries, r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matern_details(kernel):\n",
    "    return [kernel,kernel.length_scale, kernel.nu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_details(kernel):\n",
    "    return [kernel, kernel.length_scale, kernel.length_scale_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationalquadratic_details(kernel):\n",
    "    return [kernel, kernel.length_scale, kernel.alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_plus_white(kernel):\n",
    "    return [kernel, kernel.k1.length_scale, kernel.k2.noise_level]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Matern\n",
    "- 26 minutes with `2*2*10` Matern configurations at 100 queries - 2,5 learners\n",
    "- 3 hours 4 minutes with `2*2*10` Matern configurations at 100 queries - 10,20 learners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'nu', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [Matern(length_scale=i, nu=1.5) for i in np.linspace(0.5,1,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, matern_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.38,0.418,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. RationalQuadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'alpha', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RationalQuadratic(length_scale=i, alpha=j, length_scale_bounds=(1e-2, 1e3)) \n",
    "#            for i in np.linspace(0.5,10,2) \n",
    "#            for j in np.linspace(0.5,2,2)]\n",
    "\n",
    "# n_learners = [2]\n",
    "# n_initials = [10]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rationalquadratic_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. RBF + WhiteKernel random search\n",
    "Stopped halfway through because results weren't interesting. Adding noise doesn't improve score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','rbf_length_scale', 'white_noise_scale', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=0.45,length_scale_bounds=(1e-2, 1e3)) \\\n",
    "#            + WhiteKernel(noise_level=i, noise_level_bounds=(1e-10, 1e+1)) \n",
    "#            for i in np.random.uniform(low=0.1,high=1.0,size=20)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_plus_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. RBF linear search wide\n",
    "Search for peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in [1e-10,1e-5,1e-2,1e-1,1,1e1,100]]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. RBF linear search narrow\n",
    "Search near current peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 1 hour\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in np.linspace(0.1,1,10)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(1,10,10)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. RBF linear search narrow 2\n",
    "Search near current peaks. Skipped fifth run due to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "# row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)] \\\n",
    "#         + [RBF(length_scale=i) for i in np.linspace(0.4,7,40)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 100\n",
    "\n",
    "# # run process\n",
    "# gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. RBF linear search narrow 3\n",
    "Search near current peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF(length_scale=0.45)|10|80|100|0.48003678806113137\n",
      "RBF(length_scale=0.461)|10|80|100|0.5280713091960627\n",
      "RBF(length_scale=0.472)|10|80|100|0.5439020898579181\n",
      "RBF(length_scale=0.483)|10|80|100|0.5220100708079571\n",
      "RBF(length_scale=0.494)|10|80|100|0.5278239065898067\n",
      "RBF(length_scale=0.506)|10|80|100|0.537708895275952\n",
      "RBF(length_scale=0.517)|10|80|100|0.5488158607863621\n",
      "RBF(length_scale=0.528)|10|80|100|0.5343058715448179\n",
      "RBF(length_scale=0.539)|10|80|100|0.514719934237948\n",
      "RBF(length_scale=0.55)|10|80|100|0.5523402212766171\n",
      "RBF(length_scale=0.7)|10|80|100|0.532097956922985\n",
      "RBF(length_scale=0.711)|10|80|100|0.5519891604705784\n",
      "RBF(length_scale=0.722)|10|80|100|0.5440713417827029\n",
      "RBF(length_scale=0.733)|10|80|100|0.5362019662150257\n",
      "RBF(length_scale=0.744)|10|80|100|0.5401868681184716\n",
      "RBF(length_scale=0.756)|10|80|100|0.5445412978736106\n",
      "RBF(length_scale=0.767)|10|80|100|0.5394760505883494\n",
      "RBF(length_scale=0.778)|10|80|100|0.5517369517058294\n",
      "RBF(length_scale=0.789)|10|80|100|0.5416097938927242\n",
      "RBF(length_scale=0.8)|10|80|100|0.5341649211825974\n",
      "RBF(length_scale=2.4)|10|80|100|0.5396867957875124\n",
      "RBF(length_scale=2.47)|10|80|100|0.5526126167388499\n",
      "RBF(length_scale=2.53)|10|80|100|0.5424732517321159\n",
      "RBF(length_scale=2.6)|10|80|100|0.5508500850589\n",
      "RBF(length_scale=2.67)|10|80|100|0.5554851475584364\n",
      "RBF(length_scale=2.73)|10|80|100|0.5067504139530608\n",
      "RBF(length_scale=2.8)|10|80|100|0.525463568831062\n",
      "RBF(length_scale=2.87)|10|80|100|0.5480651772765675\n",
      "RBF(length_scale=2.93)|10|80|100|0.5535287535796973\n",
      "RBF(length_scale=3)|10|80|100|0.5336923946540032\n",
      "RBF(length_scale=3.7)|10|80|100|0.5178615655489848\n",
      "RBF(length_scale=3.73)|10|80|100|0.5444843264267303\n",
      "RBF(length_scale=3.77)|10|80|100|0.5269077120711421\n",
      "RBF(length_scale=3.8)|10|80|100|0.5473400618971845\n",
      "RBF(length_scale=3.83)|10|80|100|0.5545219909558374\n",
      "RBF(length_scale=3.87)|10|80|100|0.5407111811744805\n",
      "RBF(length_scale=3.9)|10|80|100|0.5527072448747861\n",
      "RBF(length_scale=3.93)|10|80|100|0.5511930562766252\n",
      "RBF(length_scale=3.97)|10|80|100|0.5383007930327202\n",
      "RBF(length_scale=4)|10|80|100|0.5274885330176196\n",
      "RBF(length_scale=4.8)|10|80|100|0.5193246959276523\n",
      "RBF(length_scale=4.84)|10|80|100|0.5518778388802823\n",
      "RBF(length_scale=4.89)|10|80|100|0.5387906543964776\n",
      "RBF(length_scale=4.93)|10|80|100|0.5378459006506384\n",
      "RBF(length_scale=4.98)|10|80|100|0.5530491815811701\n",
      "RBF(length_scale=5.02)|10|80|100|0.5275277231350549\n",
      "RBF(length_scale=5.07)|10|80|100|0.5339664518490485\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# results file\n",
    "filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "row_prefix = ['Committee','Gaussian Process','max_std_sampling']\n",
    "fields = ['category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# configs\n",
    "kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.45,0.55,10)] \\\n",
    "        + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.7,0.8,10)] \\\n",
    "        + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(2.4,3.0,10)] \\\n",
    "        + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(3.7,4.0,10)] \\\n",
    "        + [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(4.8,5.2,10)]  \n",
    "\n",
    "n_learners = [10]\n",
    "n_initials = [80]\n",
    "n_queries = 100\n",
    "\n",
    "# run process\n",
    "gaussian_process_regressor_gs(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "                                        seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. RBF linear search narrow 4\n",
    "Search near current peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_id = ['3.9']\n",
    "row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "row_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF(length_scale=0.45)|10|80|100|0.48003678806113137\n",
      "RBF(length_scale=0.461)|10|80|100|0.5280713091960627\n",
      "RBF(length_scale=0.472)|10|80|100|0.5439020898579181\n",
      "RBF(length_scale=0.483)|10|80|100|0.5220100708079571\n",
      "RBF(length_scale=0.494)|10|80|100|0.5278239065898067\n",
      "RBF(length_scale=0.506)|10|80|100|0.537708895275952\n",
      "RBF(length_scale=0.517)|10|80|100|0.5488158607863621\n",
      "RBF(length_scale=0.528)|10|80|100|0.5343058715448179\n",
      "RBF(length_scale=0.539)|10|80|100|0.514719934237948\n",
      "RBF(length_scale=0.55)|10|80|100|0.5523402212766171\n",
      "RBF(length_scale=0.7)|10|80|100|0.532097956922985\n",
      "RBF(length_scale=0.711)|10|80|100|0.5519891604705784\n",
      "RBF(length_scale=0.722)|10|80|100|0.5440713417827029\n",
      "RBF(length_scale=0.733)|10|80|100|0.5362019662150257\n",
      "RBF(length_scale=0.744)|10|80|100|0.5401868681184716\n",
      "RBF(length_scale=0.756)|10|80|100|0.5445412978736106\n",
      "RBF(length_scale=0.767)|10|80|100|0.5394760505883494\n",
      "RBF(length_scale=0.778)|10|80|100|0.5517369517058294\n",
      "RBF(length_scale=0.789)|10|80|100|0.5416097938927242\n",
      "RBF(length_scale=0.8)|10|80|100|0.5341649211825974\n",
      "RBF(length_scale=2.4)|10|80|100|0.5396867957875124\n",
      "RBF(length_scale=2.47)|10|80|100|0.5526126167388499\n",
      "RBF(length_scale=2.53)|10|80|100|0.5424732517321159\n",
      "RBF(length_scale=2.6)|10|80|100|0.5508500850589\n",
      "RBF(length_scale=2.67)|10|80|100|0.5554851475584364\n",
      "RBF(length_scale=2.73)|10|80|100|0.5067504139530608\n",
      "RBF(length_scale=2.8)|10|80|100|0.525463568831062\n",
      "RBF(length_scale=2.87)|10|80|100|0.5480651772765675\n",
      "RBF(length_scale=2.93)|10|80|100|0.5535287535796973\n",
      "RBF(length_scale=3)|10|80|100|0.5336923946540032\n",
      "RBF(length_scale=3.7)|10|80|100|0.5178615655489848\n",
      "RBF(length_scale=3.73)|10|80|100|0.5444843264267303\n",
      "RBF(length_scale=3.77)|10|80|100|0.5269077120711421\n",
      "RBF(length_scale=3.8)|10|80|100|0.5473400618971845\n",
      "RBF(length_scale=3.83)|10|80|100|0.5545219909558374\n",
      "RBF(length_scale=3.87)|10|80|100|0.5407111811744805\n",
      "RBF(length_scale=3.9)|10|80|100|0.5527072448747861\n",
      "RBF(length_scale=3.93)|10|80|100|0.5511930562766252\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # results file\n",
    "# filename = 'data/gridsearch.' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "\n",
    "# ex_id = ['3.9']\n",
    "# row_prefix = ex_id + ['Committee','Gaussian Process','max_std_sampling']\n",
    "# fields = ['experiment','category','learner','query_strategy','kernel','length_scale', 'length_scale_bounds', 'n_learners', 'n_initial','n_queries','r2']\n",
    "\n",
    "# # configs\n",
    "# kernels = [RBF(length_scale=i,length_scale_bounds=(1e-2, 1e3)) for i in np.linspace(0.45,0.55,10)]\n",
    "\n",
    "# n_learners = [10]\n",
    "# n_initials = [80]\n",
    "# n_queries = 1\n",
    "\n",
    "# # run process\n",
    "# grid_search_1(kernels,n_learners,n_initials,X_train,y_train,X_test,y_test,n_queries,\n",
    "#                                         seed, filename, fields, row_prefix, rbf_details)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
