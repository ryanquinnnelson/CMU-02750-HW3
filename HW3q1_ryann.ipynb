{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "# sklearn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modAL\n",
    "from modAL.disagreement import max_std_sampling\n",
    "from modAL.models import ActiveLearner, CommitteeRegressor\n",
    "\n",
    "# scipy\n",
    "from scipy import sparse\n",
    "\n",
    "# custom package\n",
    "from packages.activelearning import activelearning as al\n",
    "\n",
    "\n",
    "### Set random seed\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "### Suppresses Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. Data Load (5 points)\n",
    "**In this question, you need to build a regression model with active learning for predicting binding affinity between MHC class I and small peptides. The dataset is provided in file `hw3_data.csv`.**\n",
    "\n",
    "**TODO**\n",
    "- **Read the data into the jupyter notebook. Columns 2 and 3 in the dataset file correspond to peptide sequences and pIC50 values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>allele</th>\n",
       "      <th>seq</th>\n",
       "      <th>pIC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seq0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>AIIDYIAYM</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seq1</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>AIYDTMQYV</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seq2</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>ALATFTVNI</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seq3</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>ALDEGLLPV</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seq4</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>ALFPIIWAL</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id       allele        seq  pIC50\n",
       "0  seq0  HLA-A*02:01  AIIDYIAYM    9.0\n",
       "1  seq1  HLA-A*02:01  AIYDTMQYV    9.0\n",
       "2  seq2  HLA-A*02:01  ALATFTVNI    9.0\n",
       "3  seq3  HLA-A*02:01  ALDEGLLPV    9.0\n",
       "4  seq4  HLA-A*02:01  ALFPIIWAL    9.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/hw3_data.csv', delimiter=',',header=0)\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Encode Data (10 points)\n",
    "**Since we are dealing with machine learning models, you need to convert peptide sequences into feature vectors. The simplest way to do this is to use a one-hot encoding.**\n",
    "\n",
    "**Each character in the amino acid alphabet will correspond to a binary vector with a single 1 and all zeros. The size of the vector is equal to the size of the amino acid alphabet. The position of 1 encodes a specific amino acid. The resulting feature vector for a peptide is a concatenation of the feature vectors of its amino acids. Since we are dealing with 9-mers here, the size of the feature vector for a peptide should be equal to 9*(size of the amino acid alphabet).**\n",
    "\n",
    "**TODO**\n",
    "- **Encode the data.**\n",
    "- **Split data into train and test datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['seq'].str.len().unique()  # every seq is length 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate columns for each amino acid\n",
    "for i in range(9):\n",
    "    colname='seq'+str(i)\n",
    "    data[colname] = [x[i] for x in data['seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>allele</th>\n",
       "      <th>seq</th>\n",
       "      <th>pIC50</th>\n",
       "      <th>seq0</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "      <th>seq3</th>\n",
       "      <th>seq4</th>\n",
       "      <th>seq5</th>\n",
       "      <th>seq6</th>\n",
       "      <th>seq7</th>\n",
       "      <th>seq8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seq0</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>AIIDYIAYM</td>\n",
       "      <td>9.0</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>D</td>\n",
       "      <td>Y</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id       allele        seq  pIC50 seq0 seq1 seq2 seq3 seq4 seq5 seq6  \\\n",
       "0  seq0  HLA-A*02:01  AIIDYIAYM    9.0    A    I    I    D    Y    I    A   \n",
       "\n",
       "  seq7 seq8  \n",
       "0    Y    M  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique letters\n",
    "len(data['seq0'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 9)\n",
      "(9051,)\n"
     ]
    }
   ],
   "source": [
    "# split into features and target\n",
    "X_df = data.drop(['pIC50','id','allele', 'seq'],axis=1) # eliminate columns which are irrelevant\n",
    "y = data['pIC50']\n",
    "print(X_df.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq0</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "      <th>seq3</th>\n",
       "      <th>seq4</th>\n",
       "      <th>seq5</th>\n",
       "      <th>seq6</th>\n",
       "      <th>seq7</th>\n",
       "      <th>seq8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>D</td>\n",
       "      <td>Y</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  seq0 seq1 seq2 seq3 seq4 seq5 seq6 seq7 seq8\n",
       "0    A    I    I    D    Y    I    A    Y    M"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(9051, 180)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_enc = enc.fit_transform(X_df)\n",
    "print(type(X_enc))\n",
    "print(X_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(9051, 180)\n",
      "(9051,)\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array\n",
    "X_pool=sparse.csr_matrix.toarray(X_enc)\n",
    "print(type(X_pool))\n",
    "print(X_pool.shape)\n",
    "y_pool = y.to_numpy()\n",
    "print(y_pool.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "- X_train: 2/3 of data pool\n",
    "- X_test: 1/3 of data pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6064, 180)\n",
      "(6064,)\n",
      "(2987, 180)\n",
      "(2987,)\n"
     ]
    }
   ],
   "source": [
    "# split remaining pool of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pool, y_pool, test_size=0.33)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Build Active Learning  model (20 points)\n",
    "**Now you can start building the model with active learning! We do not give you any specific instructions as to which algorithm to implement. You can use something similar to [this](https://modal-python.readthedocs.io/en/latest/content/examples/active_regression.html) example in modAL documentation, or query-by-committee when a label is requested for samples which the committee is least confident about or anything else covered in the lectures.**\n",
    "\n",
    "**TODO**\n",
    "- **Implement an active learning algorithm using modAL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_to_file(filename, history):\n",
    "    '''\n",
    "    Writes history to file.\n",
    "    \n",
    "    Args:\n",
    "        filename: name of the file\n",
    "        history: list of scores\n",
    "    ''' \n",
    "    df = pd.DataFrame({'r2': history} )  \n",
    "    df.to_csv(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21h 51min 39s, sys: 9min 51s, total: 22h 1min 31s\n",
      "Wall time: 6h 30min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 56 min for 400 with 10 learners, length_scale=0.5, 0.475 at 100 queries\n",
    "# 50 min for 400 with 10 learners, length_scale=2.769, 0.560 at 100 queries (test)\n",
    "# 6.5 hours for 1000 with 10 learners, length_scale=2.769\n",
    "\n",
    "# make a copy of the data for use in this section\n",
    "X_pool_rbf = copy.deepcopy(X_train)\n",
    "y_pool_rbf = copy.deepcopy(y_train)\n",
    "\n",
    "# build active learner\n",
    "kernel = RBF(length_scale=2.769,length_scale_bounds=(1e-2, 1e3))\n",
    "n_learner = 10\n",
    "n_initial = 80\n",
    "committee_rbf = al.build_committee(kernel, n_learner, n_initial, X_pool_rbf, y_pool_rbf, seed)\n",
    "\n",
    "# perform active learning\n",
    "n_queries = 1000\n",
    "history_rbf = al.run_and_score_active_learner_regression(committee_rbf, X_pool_rbf, y_pool_rbf, X_test, y_test, n_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "filename = 'data/fullrun.rbf_2.769_' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "save_history_to_file(filename, history_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with Matern kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 1h 22min 59s, sys: 17min 50s, total: 1d 1h 40min 49s\n",
      "Wall time: 7h 35min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 58 min for 400 with 10 learners\n",
    "# 7.5 hours\n",
    "\n",
    "# make a copy of the data for use in this section\n",
    "X_pool_mat = copy.deepcopy(X_train)\n",
    "y_pool_mat = copy.deepcopy(y_train)\n",
    "\n",
    "# build active learner\n",
    "kernel = Matern(length_scale=0.64, nu=1.5)\n",
    "n_learner = 10\n",
    "n_initial = 80\n",
    "committee_mat = al.build_committee(kernel, n_learner, n_initial, X_pool_mat, y_pool_mat, seed)\n",
    "\n",
    "# perform active learning\n",
    "n_queries = 1000\n",
    "history_mat = al.run_and_score_active_learner_regression(committee_mat, X_pool_mat, y_pool_mat, X_test, y_test, n_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "filename = 'data/fullrun.matern_0.64_' + datetime.datetime.now().strftime('%Y.%m.%d.%H.%M.%S') + '.csv'\n",
    "save_history_to_file(filename, history_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Score model (5 points)\n",
    "**The quality of the model should be measured on the test set as $R^2$ score. The minimum acceptable $R^2$ score is 0.6.**\n",
    "\n",
    "**TODO**\n",
    "- **Score the model on the test set using $R^2$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test set 0.6126014746782097\n",
      "R2 for pool 0.677571218738872\n"
     ]
    }
   ],
   "source": [
    "# score the final model\n",
    "r2 = al.score_regression_model(committee_rbf, X_test,y_test)\n",
    "print(\"R2 for test set\",r2) # 0.6126014746782097\n",
    "\n",
    "\n",
    "# compare against R2 of the remaining pool\n",
    "r2_pool = al.score_regression_model(committee_rbf, X_pool_rbf,y_pool_rbf)\n",
    "print(\"R2 for pool\",r2_pool) # 0.677571218738872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Matern kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test set 0.5989614229925406\n",
      "R2 for pool 0.6665529679520565\n"
     ]
    }
   ],
   "source": [
    "# score the final model\n",
    "r2 = al.score_regression_model(committee_mat, X_test,y_test)\n",
    "print(\"R2 for test set\",r2)\n",
    "\n",
    "\n",
    "# compare against R2 of the remaining pool\n",
    "r2_pool = al.score_regression_model(committee_mat, X_pool_mat,y_pool_mat)\n",
    "print(\"R2 for pool\",r2_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5. Compare with Random Active Learner (5 points)\n",
    "**TODO**\n",
    "- **Compare your results with no active learning scheme by training a random forest regressor on the same amount of data points, but selected randomly. (An active learner with a random query strategy.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RfWrapper(RandomForestRegressor):  # superclass\n",
    "    def predict(self, X, return_std = False):\n",
    "        if return_std:\n",
    "            ys = np.array([e.predict(X) for e in self.estimators_])\n",
    "            return np.mean(ys, axis = 0).ravel(), np.std(ys, axis = 0).ravel()\n",
    "        return super().predict(X).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_query(learner, X):\n",
    "   \n",
    "    n_samples = len(X)\n",
    "    query_idx = np.random.choice(range(n_samples))\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run random learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2.5 min\n",
    "\n",
    "# copy data for this section\n",
    "X_pool_rand = copy.deepcopy(X_train)\n",
    "y_pool_rand = copy.deepcopy(y_train)\n",
    "\n",
    "\n",
    "# use the same initialization size to make comparison meaningful\n",
    "n_initial = 80\n",
    "initial_idx = []\n",
    "for i in range(1):\n",
    "    initial_idx.append(np.random.choice(len(X_pool_rand), size=n_initial, replace=False))\n",
    "initial_rand = initial_idx[0]\n",
    "\n",
    "regressor = ActiveLearner(\n",
    "    estimator=RfWrapper(n_estimators = 66, max_depth = 9, random_state = seed),\n",
    "    query_strategy=random_query,\n",
    "    X_training=X_pool_rand[initial_rand], \n",
    "    y_training=y_pool_rand[initial_rand]\n",
    ")\n",
    "\n",
    "\n",
    "# run learner\n",
    "n_queries = 1000\n",
    "history_rand = al.run_and_score_active_learner_regression(regressor, X_pool_rand, y_pool_rand, X_test, y_test, n_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score random learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the final model\n",
    "\n",
    "r2=al.score_regression_model(regressor, X_test,y_test)\n",
    "print(\"R2 for test set\",r2)\n",
    "\n",
    "# compare against R2 of the remaining pool\n",
    "r2_pool=al.score_regression_model(regressor, X_pool_rand,y_pool_rand)\n",
    "print(\"R2 for pool\",r2_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against active learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two results\n",
    "fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n",
    "\n",
    "ax.plot(history_rbf)\n",
    "ax.scatter(range(len(history_rbf)), history_rbf, s=13, label = 'Committee RBF')\n",
    "\n",
    "ax.plot(history_rand)\n",
    "ax.scatter(range(len(history_rand)), history_rand, s=13, label = 'Random RF')\n",
    "\n",
    "ax.plot(history_mat)\n",
    "ax.scatter(range(len(history_mat)), history_mat, s=13, label = 'Committee Matern')\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_title('Incremental regression accuracy')\n",
    "ax.set_xlabel('Query iteration')\n",
    "ax.set_ylabel('R2 Score')\n",
    "ax.set_ylim([0.2,.65])\n",
    "ax.axhline(y=0.6, color='red', linestyle='--', label='User-defined threshold')\n",
    "\n",
    "ax.legend()\n",
    "plt.yticks(np.arange(.1, 0.65, step=0.025)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6. Summary (5 points)\n",
    "**TODO**\n",
    "- **Write a paragraph about your method, describe your observations and difficulties.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "I encoded the data by separating the sequence into individual letters and performing one-hot encoding. I removed all all feature columns because this data was for a single HLA allele. I did not standardize the pIC50 values.\n",
    "\n",
    "I experimented with a number of regressors, and I chose to use Gaussian Process because it was the highest performing regressor on the full training set. I also experimented with a number of kernel combinations, and I chose to use RBF and Matern kernels because they were the highest performing kernels on the full training set. I used a committee-based approach because I anticipated that using only a small amount of training data during active learning meant each model would be a weak regressor. I predicted a committee-based approach would combine the efforts of multiple models and result in an overall better model. I chose to have all committee members be the same type of active learner, and I used standard deviation sampling as the query strategy.\n",
    "\n",
    "The next step was to tune the model's hyperparameters. I experimented with the following hyperparameters: (1) number of committee members; (2) number of initial training instances for each active learner; (3) all kernel parameters. \n",
    "\n",
    "Using a grid search, I found that increasing the number of committee members from 2 to 10 improved $R^2$ scores, and that after 10 members, gains were neglible. I found that increasing the number of initial training instances greatly improved $R^2$ scores, but any gains past `n_initial=80` were neglible. For the RBF kernel, I tuned `length_scale` and `length_scale_bounds`. I found that decreasing `length_scale_bounds` allowed the model to run faster, but had little effect on the scores. After extensive searching over `length_scale`, I chose a value which was associated with the consistently high $R^2$ scores (above 0.55 at 100 queries). For the Matern kernel, I tuned `length_scale` and `nu`. I found that modifying `nu` from its default value of 1.5 led to decreased performance. After searching over `length_scale`, I chose a value which was associated with the consistently high $R^2$ scores (above 0.52 at 100 queries).\n",
    "\n",
    "I ran the final model using a commitee of Gaussian Process regressors which all had the same kernel, and compared performance against the random active learner. Note that I set the hyperparameters of the random forest regressor based on a grid search.\n",
    "\n",
    "![image.png](img/gridsearch2/4.3_estimators_vs_max_depth_heatmap.png)\n",
    "\n",
    "\n",
    "### Observations\n",
    "There are a number of observations which could be made about the process and the results.\n",
    "\n",
    "#### Process\n",
    "- I found it was very important to save to file all results from the grid search and organize them according to experiment run. Without a record of what had been tried, it was possible to duplicate efforts. If results weren't organized, it was difficult to tell whether progress was being made and how to explain the narrative of how you arrived at your results.\n",
    "- I found it was important to fix all other hyperparameters in order to study how each hyperparameter changed. For example, if I wanted to understand how modifying `length_scale` of my kernel affected $R^2$ scores, I needed to eliminate the possibility that changes to the score were related to size of the initial training set or number of committee members.\n",
    "- I found it was important to begin with a wide grid search before narrowing into specific ranges. Without this, I might fail to consider values in a region which could contain the global optimum. \n",
    "- I found it was important to using a smaller number of queries for hyperparameter tuning. A small number of queries meant that I could perform more experiments to determine the best values.\n",
    "\n",
    "#### Results\n",
    "- With Gaussian Process using the RBF kernel, I was able to achieve $R^2$ within 315 queries. With Gaussian process using the Matern kernel, it took 971 queries to achieve the same. Both RBF and Matern kernels performed well, with Matern seeing greater improvement over time.\n",
    "- I found that the active learner performed better than the random learner, but much of this had to do with the starting point. The random learner learned at an approximately linear rate, whereas both active learners had exponential early learning followed by asymptotic improvement later. The random forest regressor couldn't achieve the same high starting score using the same number of initial training instances.\n",
    "\n",
    "\n",
    "### Difficulties\n",
    "I faced a number of challenges in this process. \n",
    "- I do not have a background in active learning regression or kernels, so I didn't have intuition as to which regressor or kernel might work well. There are too many possible options to approach this problem through brute force alone, so I often had to settle for \"good enough\" to keep moving forward. \n",
    "- Tuning kernel hyperparameters proved very challenging. Kernel parameters are real-valued (infinite range of values), runs took a long time (approx. 10-20 minutes per 100 queries), and resulting scores were somewhat stochastic even after using a random seed. I could only perform a small number of runs with a limited number of queries, and I needed to be very selective about the kernel parameter ranges I studied. After collecting a large amount of results, it was hard to clearly discern which values were optimal. It was also difficult to determine whether the results I was seeing after 100 queries were predictive of model performance at 200 queries or higher. I failed to achieve an $R^2$ score of 0.60 during searching, so I could only anticipate which values would achieve the desired result. I tried over 600 different combinations of hyperparameters and only made a small dent in the search space. Once again, I had to settle for \"good enough\" to keep moving forward.\n",
    "![image.png](img/gridsearch1/3.10_scatterplot_all.png)\n",
    "- The stochastic nature of the Gaussian Process made it difficult to reproduce results I had seen during testing. For example, with my first full run with the RBF kernel, I got 0.475 at 100 queries when I was able to achieve 0.55 on 100 queries during testing. Previous runs using the same length_scale with which I exceeded $R^2=0.6$ only achieved 0.58.\n",
    "- The biggest challenge was determining whether I could further improve $R^2$ after reaching the limits of the current model. There appeared to be a clear asymptotic limit to the performance of my chosen models. In other words, hyperparameter tuning could only do so much. In order to make significant improvements, I would likely need to change my approach entirely. But would it make a difference? Any attempts at this seemed like a shot in the dark - there were too many options: Should I alter the encoding of my data to include relationship between amino acids (i.e. protein shape)? Should I use a different regressor? Should I use a single model instead of a committee? Should I be using a different query strategy? Long run times meant I could not try everything.\n",
    "\n",
    "\n",
    "### Follow-Up\n",
    "It may be possible to improve the results further by encoding the order of the amino acids. The 3D structure they form is vital for binding, and structure is determined by sequence order. One possible way is to group letters in order. How do I decide how many letters constitutes a group?\n",
    "\n",
    "Ex: 3-mers\n",
    "```\n",
    "AIIDYIAYM\n",
    "AII\n",
    " IID\n",
    "  IDY\n",
    "    ...\n",
    "      AYM\n",
    "```\n",
    "\n",
    "3-mers results in N < J, where N is the number of samples and J is the number of features. If we want to use multivariate regression, we'll need to use ridge regression or some other sparse form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create triplets\n",
    "# k = 3\n",
    "# count = 9-k+1\n",
    "# start=0\n",
    "# end=k\n",
    "# for i in range(count):\n",
    "#     colname='triplet'+str(i)\n",
    "#     data[colname] = [x[start:end] for x in data['seq']]\n",
    "#     start +=1\n",
    "#     end +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
